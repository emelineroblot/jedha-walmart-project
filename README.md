# ğŸ›’ Walmart Sales Prediction

Projet de **Machine Learning supervisÃ©** pour prÃ©dire les ventes hebdomadaires des magasins Walmart en utilisant des indicateurs Ã©conomiques.

---

## ğŸ“‹ Description du projet

Walmart souhaite construire un modÃ¨le de Machine Learning capable d'estimer les ventes hebdomadaires dans ses magasins avec la meilleure prÃ©cision possible. Ce modÃ¨le aide Ã  :
- Comprendre comment les ventes sont influencÃ©es par les indicateurs Ã©conomiques
- Planifier les futures campagnes marketing

---

## ğŸ¯ Objectifs

Le projet est divisÃ© en trois parties :

1. **Partie 1** : Analyse exploratoire (EDA) et preprocessing des donnÃ©es
2. **Partie 2** : EntraÃ®ner un modÃ¨le de **rÃ©gression linÃ©aire** (baseline)
3. **Partie 3** : RÃ©duire l'overfitting avec des modÃ¨les **rÃ©gularisÃ©s** (Ridge et Lasso)

---

## ğŸ“Š Dataset

Le dataset contient **150 lignes** et **8 colonnes** :

| Colonne | Type | Description |
|---------|------|-------------|
| `Store` | CatÃ©gorielle | Identifiant du magasin (1-20) |
| `Date` | Date | Date de la semaine |
| `Weekly_Sales` | NumÃ©rique | **Target** - Ventes hebdomadaires en $ |
| `Holiday_Flag` | CatÃ©gorielle | Indicateur jour fÃ©riÃ© (0=Non, 1=Oui) |
| `Temperature` | NumÃ©rique | TempÃ©rature moyenne (Â°F) |
| `Fuel_Price` | NumÃ©rique | Prix du carburant ($) |
| `CPI` | NumÃ©rique | Indice des prix Ã  la consommation |
| `Unemployment` | NumÃ©rique | Taux de chÃ´mage (%) |

**AprÃ¨s nettoyage** : 71 lignes exploitables (suppression des NaN et outliers).

---

## ğŸ› ï¸ Ã‰tapes du projet

### Partie 1 : EDA et Preprocessing

#### 1. Exploration des donnÃ©es
- Chargement du dataset
- Statistiques descriptives (`.info()`, `.describe()`)
- Analyse des valeurs manquantes (~8-12% par colonne)
- Visualisations :
  - Distribution de `Weekly_Sales`
  - Matrice de corrÃ©lation
  - Boxplots pour dÃ©tecter les outliers
  - Ventes moyennes par magasin
  - Impact des jours fÃ©riÃ©s sur les ventes

#### 2. Nettoyage des donnÃ©es
- Suppression des lignes oÃ¹ `Weekly_Sales` est NaN (14 lignes)
- Transformation de la colonne `Date` en 4 features numÃ©riques :
  - `Year`
  - `Month`
  - `Day`
  - `DayOfWeek` (0=Lundi, 6=Dimanche)
- Suppression des lignes avec NaN restants
- **Suppression des outliers** avec la rÃ¨gle des 3-sigma (mean Â± 3Ã—std) sur :
  - `Temperature`
  - `Fuel_Price`
  - `CPI`
  - `Unemployment`
- **Dataset final** : 71 lignes

#### 3. PrÃ©paration pour le ML
- SÃ©paration X (features) et y (target)
- Identification des types de variables :
  - **CatÃ©gorielles** : `Store`, `Holiday_Flag`
  - **NumÃ©riques** : `Temperature`, `Fuel_Price`, `CPI`, `Unemployment`, `Year`, `Month`, `Day`, `DayOfWeek`
- Split train/test : **80/20** (56 train, 15 test)
- Preprocessing avec `ColumnTransformer` :
  - `StandardScaler` pour les variables numÃ©riques
  - `OneHotEncoder(handle_unknown='ignore')` pour les catÃ©gorielles

---

### Partie 2 : RÃ©gression LinÃ©aire (Baseline)

#### 1. EntraÃ®nement
- ModÃ¨le : `LinearRegression()`
- PrÃ©dictions sur train et test

#### 2. Ã‰valuation
MÃ©triques utilisÃ©es :
- **RMSE** (Root Mean Squared Error) : erreur moyenne en dollars
- **MAE** (Mean Absolute Error) : erreur absolue moyenne
- **RÂ²** (Coefficient de dÃ©termination) : variance expliquÃ©e (0 Ã  1)

#### 3. InterprÃ©tation des coefficients
- Extraction des coefficients avec `.coef_`
- Identification des features les plus importantes
- Visualisation avec barplot horizontal

---

### Partie 3 : RÃ©gularisation (Ridge et Lasso)

#### 1. Ridge Regression
- ModÃ¨le : `Ridge(alpha=1.0)`
- PÃ©nalise les gros coefficients pour rÃ©duire l'overfitting
- Ã‰valuation sur train et test

#### 2. Lasso Regression
- ModÃ¨le : `Lasso(alpha=1.0)`
- Peut mettre certains coefficients Ã  zÃ©ro (sÃ©lection de features automatique)
- Affichage des features Ã©liminÃ©es

#### 3. GridSearchCV (Bonus)
- Optimisation du paramÃ¨tre `alpha` par validation croisÃ©e (5 folds)
- **Ridge** : test de 7 valeurs [0.001, 0.01, 0.1, 1, 10, 100, 1000]
- **Lasso** : test de 6 valeurs [0.001, 0.01, 0.1, 1, 10, 100]
- SÃ©lection automatique du meilleur `alpha`

#### 4. Comparaison finale
Tableau comparatif des 5 modÃ¨les :
1. Linear Regression
2. Ridge (alpha=1)
3. Lasso (alpha=1)
4. Ridge optimisÃ© (GridSearch)
5. Lasso optimisÃ© (GridSearch)

Visualisation comparative des RÂ² et RMSE.

---

## ğŸš€ Installation et utilisation

### PrÃ©requis
```bash
pip install pandas numpy matplotlib seaborn scikit-learn jupyter
```

### Lancer le projet
```bash
cd walmart_project
jupyter notebook 01-Walmart_sales.ipynb
```

### ExÃ©cution
Dans Jupyter :
1. **Kernel > Restart & Run All** pour exÃ©cuter toutes les cellules
2. Les rÃ©sultats s'affichent sÃ©quentiellement avec visualisations

---

## ğŸ“ˆ RÃ©sultats attendus

Les modÃ¨les rÃ©gularisÃ©s (Ridge/Lasso) devraient :
- RÃ©duire l'Ã©cart entre les performances train et test
- AmÃ©liorer la gÃ©nÃ©ralisation
- Obtenir un RÂ² entre 0.6 et 0.9 selon la qualitÃ© des donnÃ©es

Les features importantes identifiÃ©es devraient inclure :
- L'identifiant du magasin (`Store`)
- Les indicateurs Ã©conomiques (CPI, Unemployment)
- Les caractÃ©ristiques temporelles (Month, Year)

---

## ğŸ“ Structure du projet

```
walmart_project/
â”‚
â”œâ”€â”€ Walmart_Store_sales.csv          # Dataset
â”œâ”€â”€ 01-Walmart_sales.ipynb            # Notebook principal
â”œâ”€â”€ Contexte_projet.txt               # Brief du projet
â””â”€â”€ README.md                         # Documentation
```

---

## ğŸ” Points clÃ©s techniques

### Gestion des catÃ©gories inconnues
```python
OneHotEncoder(handle_unknown='ignore')
```
Avec un petit dataset (71 lignes) et 20 magasins, certains stores peuvent n'apparaÃ®tre que dans le test set. Le paramÃ¨tre `handle_unknown='ignore'` Ã©vite les erreurs en crÃ©ant des vecteurs de zÃ©ros.

### RÃ¨gle des 3-sigma
```python
lower_bound = mean - 3 * std
upper_bound = mean + 3 * std
```
Les valeurs hors de cet intervalle sont considÃ©rÃ©es comme outliers (99.7% des donnÃ©es normales sont dans cet intervalle).

### RÃ©gularisation
- **Ridge (L2)** : pÃ©nalise la somme des carrÃ©s des coefficients â†’ rÃ©duit leur magnitude
- **Lasso (L1)** : pÃ©nalise la somme des valeurs absolues â†’ peut mettre des coefficients exactement Ã  0

---

## ğŸ“š Ressources

- [Scikit-learn LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)
- [Scikit-learn Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)
- [Scikit-learn Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)
- [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)

---

## ğŸ“ Livrables

âœ… Visualisations (EDA)
âœ… ModÃ¨le de rÃ©gression linÃ©aire
âœ… Ã‰valuation avec mÃ©triques pertinentes (RMSE, MAE, RÂ²)
âœ… InterprÃ©tation des coefficients
âœ… ModÃ¨les rÃ©gularisÃ©s (Ridge et Lasso)
âœ… Optimisation GridSearchCV (bonus)
âœ… Comparaison finale des modÃ¨les
